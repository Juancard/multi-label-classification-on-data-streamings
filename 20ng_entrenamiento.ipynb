{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "20ng_entrenamiento.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO1L2G6hm13BeuW0ms2qSlq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Juancard/multi-label-classification-on-data-streamings/blob/master/20ng_entrenamiento.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZ32YPRWx7Kb",
        "colab_type": "text"
      },
      "source": [
        "# Dependencias"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQBAO9S-y9uO",
        "colab_type": "text"
      },
      "source": [
        "##Instalacion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXYOscwUxMJm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "5777b85a-f405-4e67-de46-d0ad741537c9"
      },
      "source": [
        "!pip install --upgrade pip\n",
        "!pip install scikit-multilearn\n",
        "!pip install liac-arff\n",
        "!pip install requests\n",
        "!pip install seaborn\n",
        "!pip install scikit-multiflow"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pip\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/74/38c2410d688ac7b48afa07d413674afc1f903c1c1f854de51dc8eb2367a5/pip-20.2-py2.py3-none-any.whl (1.5MB)\n",
            "\r\u001b[K     |▏                               | 10kB 17.5MB/s eta 0:00:01\r\u001b[K     |▍                               | 20kB 6.6MB/s eta 0:00:01\r\u001b[K     |▋                               | 30kB 7.6MB/s eta 0:00:01\r\u001b[K     |▉                               | 40kB 7.6MB/s eta 0:00:01\r\u001b[K     |█                               | 51kB 6.6MB/s eta 0:00:01\r\u001b[K     |█▎                              | 61kB 7.4MB/s eta 0:00:01\r\u001b[K     |█▌                              | 71kB 7.9MB/s eta 0:00:01\r\u001b[K     |█▊                              | 81kB 8.6MB/s eta 0:00:01\r\u001b[K     |██                              | 92kB 8.4MB/s eta 0:00:01\r\u001b[K     |██▏                             | 102kB 8.8MB/s eta 0:00:01\r\u001b[K     |██▍                             | 112kB 8.8MB/s eta 0:00:01\r\u001b[K     |██▋                             | 122kB 8.8MB/s eta 0:00:01\r\u001b[K     |██▉                             | 133kB 8.8MB/s eta 0:00:01\r\u001b[K     |███                             | 143kB 8.8MB/s eta 0:00:01\r\u001b[K     |███▎                            | 153kB 8.8MB/s eta 0:00:01\r\u001b[K     |███▌                            | 163kB 8.8MB/s eta 0:00:01\r\u001b[K     |███▊                            | 174kB 8.8MB/s eta 0:00:01\r\u001b[K     |████                            | 184kB 8.8MB/s eta 0:00:01\r\u001b[K     |████▏                           | 194kB 8.8MB/s eta 0:00:01\r\u001b[K     |████▍                           | 204kB 8.8MB/s eta 0:00:01\r\u001b[K     |████▋                           | 215kB 8.8MB/s eta 0:00:01\r\u001b[K     |████▉                           | 225kB 8.8MB/s eta 0:00:01\r\u001b[K     |█████                           | 235kB 8.8MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 245kB 8.8MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 256kB 8.8MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 266kB 8.8MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 276kB 8.8MB/s eta 0:00:01\r\u001b[K     |██████                          | 286kB 8.8MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 296kB 8.8MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 307kB 8.8MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 317kB 8.8MB/s eta 0:00:01\r\u001b[K     |███████                         | 327kB 8.8MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 337kB 8.8MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 348kB 8.8MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 358kB 8.8MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 368kB 8.8MB/s eta 0:00:01\r\u001b[K     |████████                        | 378kB 8.8MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 389kB 8.8MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 399kB 8.8MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 409kB 8.8MB/s eta 0:00:01\r\u001b[K     |█████████                       | 419kB 8.8MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 430kB 8.8MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 440kB 8.8MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 450kB 8.8MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 460kB 8.8MB/s eta 0:00:01\r\u001b[K     |██████████                      | 471kB 8.8MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 481kB 8.8MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 491kB 8.8MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 501kB 8.8MB/s eta 0:00:01\r\u001b[K     |███████████                     | 512kB 8.8MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 522kB 8.8MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 532kB 8.8MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 542kB 8.8MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 552kB 8.8MB/s eta 0:00:01\r\u001b[K     |████████████                    | 563kB 8.8MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 573kB 8.8MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 583kB 8.8MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 593kB 8.8MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 604kB 8.8MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 614kB 8.8MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 624kB 8.8MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 634kB 8.8MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 645kB 8.8MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 655kB 8.8MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 665kB 8.8MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 675kB 8.8MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 686kB 8.8MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 696kB 8.8MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 706kB 8.8MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 716kB 8.8MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 727kB 8.8MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 737kB 8.8MB/s eta 0:00:01\r\u001b[K     |████████████████                | 747kB 8.8MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 757kB 8.8MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 768kB 8.8MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 778kB 8.8MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 788kB 8.8MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 798kB 8.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 808kB 8.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 819kB 8.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 829kB 8.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 839kB 8.8MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 849kB 8.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 860kB 8.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 870kB 8.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 880kB 8.8MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 890kB 8.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 901kB 8.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 911kB 8.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 921kB 8.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 931kB 8.8MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 942kB 8.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 952kB 8.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 962kB 8.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 972kB 8.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 983kB 8.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 993kB 8.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 1.0MB 8.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 1.0MB 8.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 1.0MB 8.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.0MB 8.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 1.0MB 8.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 1.1MB 8.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 1.1MB 8.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.1MB 8.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.1MB 8.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 1.1MB 8.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 1.1MB 8.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 1.1MB 8.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.1MB 8.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 1.1MB 8.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 1.1MB 8.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 1.2MB 8.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 1.2MB 8.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.2MB 8.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 1.2MB 8.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 1.2MB 8.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 1.2MB 8.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.2MB 8.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 1.2MB 8.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 1.2MB 8.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 1.2MB 8.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 1.3MB 8.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.3MB 8.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 1.3MB 8.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.3MB 8.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 1.3MB 8.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.3MB 8.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 1.3MB 8.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 1.3MB 8.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.3MB 8.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.4MB 8.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.4MB 8.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.4MB 8.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.4MB 8.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 1.4MB 8.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 1.4MB 8.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.4MB 8.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.4MB 8.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.4MB 8.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.4MB 8.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.5MB 8.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 1.5MB 8.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 1.5MB 8.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 1.5MB 8.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.5MB 8.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.5MB 8.8MB/s \n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Found existing installation: pip 19.3.1\n",
            "    Uninstalling pip-19.3.1:\n",
            "      Successfully uninstalled pip-19.3.1\n",
            "Successfully installed pip-20.2\n",
            "Collecting scikit-multilearn\n",
            "  Downloading scikit_multilearn-0.2.0-py3-none-any.whl (89 kB)\n",
            "\u001b[K     |████████████████████████████████| 89 kB 4.4 MB/s \n",
            "\u001b[?25hInstalling collected packages: scikit-multilearn\n",
            "Successfully installed scikit-multilearn-0.2.0\n",
            "Collecting liac-arff\n",
            "  Downloading liac-arff-2.4.0.tar.gz (15 kB)\n",
            "Building wheels for collected packages: liac-arff\n",
            "  Building wheel for liac-arff (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for liac-arff: filename=liac_arff-2.4.0-py3-none-any.whl size=13333 sha256=41eb7a5843597ba9220215508e42d0f0da95aed8977cf97e0eb96d17e45a811b\n",
            "  Stored in directory: /root/.cache/pip/wheels/ba/2a/e1/6f7be2e2ea150e2486bff64fd6f0670f4f35f4c8f31c819fb8\n",
            "Successfully built liac-arff\n",
            "Installing collected packages: liac-arff\n",
            "Successfully installed liac-arff-2.4.0\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (2.23.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests) (2020.6.20)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.6/dist-packages (0.10.1)\n",
            "Requirement already satisfied: pandas>=0.22.0 in /usr/local/lib/python3.6/dist-packages (from seaborn) (1.0.5)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from seaborn) (1.18.5)\n",
            "Requirement already satisfied: matplotlib>=2.1.2 in /usr/local/lib/python3.6/dist-packages (from seaborn) (3.2.2)\n",
            "Requirement already satisfied: scipy>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from seaborn) (1.4.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.22.0->seaborn) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.22.0->seaborn) (2.8.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.2->seaborn) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.2->seaborn) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.2->seaborn) (1.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas>=0.22.0->seaborn) (1.15.0)\n",
            "Collecting scikit-multiflow\n",
            "  Downloading scikit_multiflow-0.5.3-cp36-cp36m-manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 5.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas>=0.25.3 in /usr/local/lib/python3.6/dist-packages (from scikit-multiflow) (1.0.5)\n",
            "Requirement already satisfied: matplotlib>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-multiflow) (3.2.2)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-multiflow) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from scikit-multiflow) (1.18.5)\n",
            "Requirement already satisfied: scikit-learn>=0.20 in /usr/local/lib/python3.6/dist-packages (from scikit-multiflow) (0.22.2.post1)\n",
            "Requirement already satisfied: sortedcontainers>=1.5.7 in /usr/local/lib/python3.6/dist-packages (from scikit-multiflow) (2.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.25.3->scikit-multiflow) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.25.3->scikit-multiflow) (2018.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.0.0->scikit-multiflow) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.0.0->scikit-multiflow) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.0.0->scikit-multiflow) (2.4.7)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.20->scikit-multiflow) (0.16.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas>=0.25.3->scikit-multiflow) (1.15.0)\n",
            "Installing collected packages: scikit-multiflow\n",
            "Successfully installed scikit-multiflow-0.5.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AiN6eVQYzCje",
        "colab_type": "text"
      },
      "source": [
        "##Dependencias"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5PiMH-ltxi4x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "ab47542d-7cd5-42dc-d2ff-753aacacf70c"
      },
      "source": [
        "import os, sys, time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from scipy import sparse\n",
        "from matplotlib import pyplot as plt\n",
        "from skmultilearn.dataset import load_dataset, load_from_arff\n",
        "from skmultilearn.utils import measure_per_label\n",
        "from sklearn.linear_model import SGDClassifier, Perceptron\n",
        "from sklearn.metrics import accuracy_score, jaccard_score, hamming_loss, precision_recall_fscore_support, log_loss\n",
        "from skmultiflow.metrics import hamming_score, exact_match, j_index\n",
        "from skmultiflow.meta.multi_output_learner import MultiOutputLearner\n",
        "from skmultiflow.meta import ClassifierChain\n",
        "from skmultiflow.trees import LabelCombinationHoeffdingTreeClassifier\n",
        "from skmultiflow.core.pipeline import Pipeline\n",
        "from skmultiflow.data.data_stream import DataStream\n",
        "from skmultiflow.evaluation.evaluate_prequential import EvaluatePrequential\n",
        "from skmultiflow.data import ConceptDriftStream, MultilabelGenerator\n",
        "\n",
        "TESIS_DIR=\"/content/drive/My Drive/tesis_cardona_mll_streaming\"\n",
        "#TESIS_DIR=\"../\""
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4VY7E5TzRy3",
        "colab_type": "text"
      },
      "source": [
        "##Funciones mias\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aw3JX970xmO2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.datasets import make_multilabel_classification\n",
        "from skmultiflow.utils import check_random_state\n",
        "class MultilabelGenerator2( MultilabelGenerator ):\n",
        "    # IGUAL QUE LA FUNCION ORIGINAL PERO NO PERMITE INSTANCIAS SIN ETIQUETAS\n",
        "    def _prepare_for_use(self):\n",
        "        print(\"Preparando Generador Multietiquetas v2\")\n",
        "        self._random_state = check_random_state(self.random_state)\n",
        "        self.X, self.y = make_multilabel_classification(n_samples=self.n_samples,\n",
        "                                                        n_features=self.n_features,\n",
        "                                                        n_classes=self.n_targets,\n",
        "                                                        n_labels=self.n_labels,\n",
        "                                                        allow_unlabeled=False, ## SE AGREGA ESTA LINEA \n",
        "                                                        random_state=self._random_state)\n",
        "        self.target_names = [\"target_\" + str(i) for i in range(self.n_targets)]\n",
        "        self.feature_names = [\"att_num_\" + str(i) for i in range(self.n_num_features)]\n",
        "        self.target_values = np.unique(self.y).tolist() if self.n_targets == 1 else \\\n",
        "            [np.unique(self.y[:, i]).tolist() for i in range(self.n_targets)]\n",
        "    # por alguna razón la clase MultilabelGenerator no implementa el método has_more_classes\n",
        "    def has_more_samples( self ):\n",
        "        return self.n_remaining_samples() > 0\n",
        "class ConceptDriftStream2(ConceptDriftStream):\n",
        "    def prob_drift( self, batch_size=1):\n",
        "        x = -4.0 * float(self.sample_idx - self.position) / float(self.width)\n",
        "        probability_drift = 1.0 / (1.0 + np.exp(x))\n",
        "        x, y = super().next_sample(batch_size)\n",
        "        return probability_drift\n",
        "    def next_sample(self, batch_size=1):\n",
        "      \"\"\"\n",
        "        Copio y pego textual de next_sample. \n",
        "        Solo quito el planchado realizado sobre la matriz de etiquetas.\n",
        "        \"self.current_sample_y.flatten()\" pasa a ser \"self.current_sample_y\"\n",
        "      \"\"\"\n",
        "      self.current_sample_x = np.zeros((batch_size, self.n_features))\n",
        "      self.current_sample_y = np.zeros((batch_size, self.n_targets))\n",
        "      for j in range(batch_size):\n",
        "          self.sample_idx += 1\n",
        "          x = -4.0 * float(self.sample_idx - self.position) / float(self.width)\n",
        "          probability_drift = 1.0 / (1.0 + np.exp(x))\n",
        "          if self._random_state.rand() > probability_drift:\n",
        "              X, y = self.stream.next_sample()\n",
        "          else:\n",
        "              X, y = self.drift_stream.next_sample()\n",
        "          self.current_sample_x[j, :] = X\n",
        "          self.current_sample_y[j, :] = y\n",
        "      return self.current_sample_x, self.current_sample_y\n",
        "\n",
        "def label_based_accuracy(y_true, y_pred, normalize=True, sample_weight=None):\n",
        "    '''\n",
        "    Compute the label-based accuracy for the multi-label case\n",
        "    http://stackoverflow.com/q/32239577/395857\n",
        "    '''\n",
        "    acc_list = []\n",
        "    for i in range(y_true.shape[0]):\n",
        "        set_true = set( np.where(y_true[i])[0] )\n",
        "        set_pred = set( np.where(y_pred[i])[0] )\n",
        "        #print('\\nset_true: {0}'.format(set_true))\n",
        "        #print('set_pred: {0}'.format(set_pred))\n",
        "        tmp_a = None\n",
        "        if len(set_true) == 0 and len(set_pred) == 0:\n",
        "            tmp_a = 1\n",
        "        else:\n",
        "            tmp_a = len(set_true.intersection(set_pred))/\\\n",
        "                    float( len(set_true.union(set_pred)) )\n",
        "        #print('tmp_a: {0}'.format(tmp_a))\n",
        "        acc_list.append(tmp_a)\n",
        "    return np.mean(acc_list)\n",
        "\n",
        "def evaluate_prequential(stream, model, pretrain_size=0.1, window_size=20):\n",
        "  stream.restart()\n",
        "  pretrain_samples = round(stream.n_remaining_samples() * pretrain_size)\n",
        "  batch_size = round((stream.n_remaining_samples() - pretrain_samples) / window_size)\n",
        "  print(\"Pretrain size (examples):\", pretrain_samples)\n",
        "  print(\"Batch size (examples):\", batch_size)\n",
        "  evaluator = EvaluatePrequential(\n",
        "      show_plot=True, \n",
        "      pretrain_size=pretrain_samples, \n",
        "      batch_size=batch_size,\n",
        "      max_samples=1000000,\n",
        "      metrics=[\"exact_match\", \"hamming_score\", \"hamming_loss\", \"j_index\", \"running_time\", \"model_size\"],\n",
        "      output_file='results_br_stream_no_drift.csv'\n",
        "  )\n",
        "  evaluator.evaluate(stream=stream, model=model)\n",
        "\n",
        "def evaluar(stream, model, pretrain_size=0.1, window_size=20):\n",
        "  stream.restart()\n",
        "  \n",
        "  pretrain_size=round(stream.n_remaining_samples() * pretrain_size)\n",
        "  print(\"Pretrain size: \", pretrain_size)\n",
        "  start_time = time.time()\n",
        "\n",
        "  # Pre training the classifier\n",
        "  X, y = stream.next_sample(pretrain_size)\n",
        "  model.partial_fit(X, y, classes=stream.target_values)\n",
        "\n",
        "  print(\"Train size: \", stream.n_remaining_samples())\n",
        "  batch_size=round(stream.n_remaining_samples() / window_size)\n",
        "  print(\"Batch size: \", batch_size)\n",
        "\n",
        "  # Keeping track of sample count, true labels and predictions to later\n",
        "  # compute the classifier's hamming score\n",
        "  iterations = 0\n",
        "  true_labels = []\n",
        "  predicts = []\n",
        "\n",
        "  while stream.has_more_samples():\n",
        "      X, y = stream.next_sample(batch_size)\n",
        "      y_pred = model.predict(X)\n",
        "      model.partial_fit(X, y)\n",
        "      predicts.extend(y_pred)\n",
        "      true_labels.extend(y)\n",
        "      iterations += 1\n",
        "  end_time = time.time()\n",
        "  print('Iterations: ', iterations)\n",
        "  print(\"Hamming score: \", hamming_score(true_labels, predicts))\n",
        "  print(\"hamming loss: \", hamming_loss(true_labels, predicts))\n",
        "  print(\"Exact Match (aka, 0/1-loss): \", exact_match(true_labels, predicts))\n",
        "  print(\"Accuracy (exampled-based, igual que exact match): \", accuracy_score(true_labels, predicts))\n",
        "  print(\"Accuracy (label-based): \", label_based_accuracy(np.array(true_labels), np.array(predicts)))\n",
        "  print(\"Accuracy per label: \", measure_per_label(accuracy_score, sparse.csr_matrix(true_labels), sparse.csr_matrix(predicts)))\n",
        "  print(\"Jaccard Index: \", j_index(true_labels, predicts))\n",
        "  print(\"Log loss: \", log_loss(true_labels, predicts))\n",
        "  print(\"precision_recall_fscore_support (samples): \", precision_recall_fscore_support(true_labels, predicts, average=\"samples\"))\n",
        "  print(\"precision_recall_fscore_support (weighted): \", precision_recall_fscore_support(true_labels, predicts, average=\"weighted\"))\n",
        "  print(\"precision_recall_fscore_support (micro): \", precision_recall_fscore_support(true_labels, predicts, average=\"micro\"))\n",
        "  print(\"precision_recall_fscore_support (macro): \", precision_recall_fscore_support(true_labels, predicts, average=\"macro\"))\n",
        "  print(\"Time: {} seconds.\".format(end_time - start_time))\n",
        "\n",
        "def label_skew_graph(y_array, color=\"red\", plot_labels=-1, print_top=10):\n",
        "  df = pd.DataFrame(y_array, columns=[i for i in range(0,y_array.shape[1])])\n",
        "  labels_set_count = df.groupby(df.columns.tolist(),as_index=False).size().sort_values(ascending=False)\n",
        "  print(\"Top \",print_top,\": \\n\", labels_set_count[:print_top])\n",
        "  labels_set_count_scaled = (labels_set_count-labels_set_count.min())/(labels_set_count.max()-labels_set_count.min())\n",
        "  sns.lineplot(data=labels_set_count_scaled.values[:plot_labels], color=color)\n",
        "\n",
        "def labels_distribution_graph(y_array, color=\"red\", print_top=5):\n",
        "  df = pd.DataFrame(y_array, columns=[i for i in range(0,y_array.shape[1])])\n",
        "  labels_distribution = df.sum(axis=1).value_counts().sort_index()\n",
        "  print(\"Número de etiquetas por instancia vs frecuencia - \",print_top, \"\\n\", labels_distribution[:print_top])\n",
        "  labels_distribution_scaled = (labels_distribution-labels_distribution.min())/(labels_distribution.max()-labels_distribution.min())\n",
        "  #print(\"Número de etiquetas por instancia vs frecuencia (escalada)\\n\", labels_distribution_scaled)\n",
        "  sns.lineplot(data=labels_distribution_scaled, color=color)\n",
        "\n",
        "def load_syn_stream(filepath, labels):\n",
        "  with open(filepath) as arff_file:\n",
        "    arff_file_content = [line.rstrip(\",\\n\") + \"\\n\" for line in arff_file]\n",
        "    with open(\"/tmp/stream\", \"w\") as f:\n",
        "      f.write(\"\".join(arff_file_content))\n",
        "  arff_path = \"/tmp/stream\"\n",
        "  label_location=\"start\"\n",
        "  arff_file_is_sparse = False\n",
        "  X_mulan, y_mulan, feature_names, label_names = load_from_arff(\n",
        "    arff_path, \n",
        "    labels,\n",
        "    label_location=label_location,\n",
        "    load_sparse=arff_file_is_sparse,\n",
        "    return_attribute_definitions=True\n",
        "  )\n",
        "  return X_mulan, y_mulan, feature_names, label_names"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1PhYS4uzIIt",
        "colab_type": "text"
      },
      "source": [
        "##Monta drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bji-IF8lxhKp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "8145e5c0-a6a0-42a8-93d2-204c91ee032d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXzNd5RKy2jP",
        "colab_type": "text"
      },
      "source": [
        "#Streams"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hg5NYP7k9QHH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_20ng_dataset():\n",
        "  arff_path = os.path.join(TESIS_DIR, \"datasets/20ng/meka/20NG-F.arff\")\n",
        "  N_LABELS = 20\n",
        "  label_location=\"start\"\n",
        "  arff_file_is_sparse = False\n",
        "  X_mulan, y_mulan, feature_names, label_names = load_from_arff(\n",
        "    arff_path, \n",
        "    N_LABELS,\n",
        "    label_location=label_location,\n",
        "    load_sparse=arff_file_is_sparse,\n",
        "    return_attribute_definitions=True\n",
        "  )\n",
        "  return X_mulan, y_mulan, feature_names, label_names"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ImkpV-lL24YY",
        "colab_type": "text"
      },
      "source": [
        "##Original"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWWeAd-nx5dT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "9b219065-6ed8-496f-c55a-0c45132fda82"
      },
      "source": [
        "X_stream, y_stream, feature_names, label_names = load_20ng_dataset()\n",
        "stream_original = DataStream(data=X_stream.todense(), y=y_stream.todense(), name=\"20ng\")\n",
        "stream_original.n_remaining_samples(), X_stream, y_stream"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(19300, <19300x1006 sparse matrix of type '<class 'numpy.float64'>'\n",
              " \twith 619079 stored elements in List of Lists format>, <19300x20 sparse matrix of type '<class 'numpy.longlong'>'\n",
              " \twith 19857 stored elements in List of Lists format>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KRQBXkLc3C3B",
        "colab_type": "text"
      },
      "source": [
        "## Sintético skmultiflow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sCrQvobOzXl7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "ad2b57bd-d521-4067-fd64-135011876430"
      },
      "source": [
        "labels_por_instancia = y_stream.sum(axis=1).mean()\n",
        "print(\"Cardinalidad:\", labels_por_instancia)\n",
        "stream_syn_skm = MultilabelGenerator2(\n",
        "  n_features=stream_original.n_features, \n",
        "  n_targets=stream_original.n_targets, \n",
        "  n_labels=1,#labels_por_instancia,\n",
        "  n_samples=stream_original.n_remaining_samples() * 2\n",
        ")\n",
        "print(\"Cantidad de instancias:\", stream_syn_skm.n_remaining_samples())\n",
        "print(\"Targets: \", stream_syn_skm.target_names)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cardinalidad: 1.028860103626943\n",
            "Preparando Generador Multietiquetas v2\n",
            "Cantidad de instancias: 38600\n",
            "Targets:  ['target_0', 'target_1', 'target_2', 'target_3', 'target_4', 'target_5', 'target_6', 'target_7', 'target_8', 'target_9', 'target_10', 'target_11', 'target_12', 'target_13', 'target_14', 'target_15', 'target_16', 'target_17', 'target_18', 'target_19']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDf_VR8O399X",
        "colab_type": "text"
      },
      "source": [
        "## Sintético MOA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_BL4dVn4Cmy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "outputId": "09d63fce-ca4e-4170-e83f-d6c886947b9a"
      },
      "source": [
        "labels = y_stream.get_shape()[1]\n",
        "filepath = os.path.join(TESIS_DIR, \"streams/20ng_syn_19300_skew0_ld25_card0_5.arff\")\n",
        "X_moa_syn, y_moa_syn, feature_names, label_names = load_syn_stream(filepath, labels)\n",
        "stream_syn_moa = DataStream(data=X_moa_syn.todense(), y=y_moa_syn.todense(), name=\"20ng_syn_moa\")\n",
        "stream_syn_moa.n_remaining_samples()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-4f3789602ea1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_stream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfilepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTESIS_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"streams/20ng_syn_19300_skew0_ld25_card0_5.arff\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mX_moa_syn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_moa_syn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_syn_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mstream_syn_moa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataStream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_moa_syn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtodense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_moa_syn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtodense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"20ng_syn_moa\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mstream_syn_moa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_remaining_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-bac85d61f3cf>\u001b[0m in \u001b[0;36mload_syn_stream\u001b[0;34m(filepath, labels)\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0mlabel_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabel_location\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0mload_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marff_file_is_sparse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m     \u001b[0mreturn_attribute_definitions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m   )\n\u001b[1;32m    156\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mX_mulan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_mulan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/skmultilearn/dataset.py\u001b[0m in \u001b[0;36mload_from_arff\u001b[0;34m(filename, label_count, label_location, input_feature_type, encode_nominal, load_sparse, return_attribute_definitions)\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mload_sparse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         arff_frame = arff.load(\n\u001b[0;32m--> 216\u001b[0;31m             \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_nominal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencode_nominal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marff\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDENSE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m         )\n\u001b[1;32m    218\u001b[0m         matrix = sparse.csr_matrix(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/arff.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(fp, encode_nominal, return_type)\u001b[0m\n\u001b[1;32m   1057\u001b[0m     \u001b[0mdecoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mArffDecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1058\u001b[0m     return decoder.decode(fp, encode_nominal=encode_nominal,\n\u001b[0;32m-> 1059\u001b[0;31m                           return_type=return_type)\n\u001b[0m\u001b[1;32m   1060\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1061\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_nominal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDENSE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/arff.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, encode_nominal, return_type)\u001b[0m\n\u001b[1;32m    890\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m             return self._decode(s, encode_nominal=encode_nominal,\n\u001b[0;32m--> 892\u001b[0;31m                                 matrix_type=return_type)\n\u001b[0m\u001b[1;32m    893\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mArffException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m             \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_current_line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/arff.py\u001b[0m in \u001b[0;36m_decode\u001b[0;34m(self, s, encode_nominal, matrix_type)\u001b[0m\n\u001b[1;32m    867\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m         \u001b[0;31m# Alter the data object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 869\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode_rows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conversors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    870\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'description'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    871\u001b[0m             \u001b[0mobj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'description'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'description'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/arff.py\u001b[0m in \u001b[0;36mdecode_rows\u001b[0;34m(self, stream, conversors)\u001b[0m\n\u001b[1;32m    516\u001b[0m     \u001b[0;34m\"\"\"Mixin to return a list from decode_rows instead of a generator\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecode_rows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconversors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 518\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_DataListMixin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode_rows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconversors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/arff.py\u001b[0m in \u001b[0;36mdecode_rows\u001b[0;34m(self, stream, conversors)\u001b[0m\n\u001b[1;32m    457\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecode_rows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconversors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 459\u001b[0;31m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_parse_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZcirqeE35xn",
        "colab_type": "text"
      },
      "source": [
        "# Clasificaciones"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jRqq0uk6lEP",
        "colab_type": "text"
      },
      "source": [
        "##Original"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1iWN61kM55K",
        "colab_type": "text"
      },
      "source": [
        "### Binary Relevance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1e0jmJ95r5T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib notebook\n",
        "classifier_br = MultiOutputLearner(\n",
        "    Perceptron()\n",
        ")\n",
        "evaluate_prequential(stream_original, classifier_br, 0.1)\n",
        "stream_original.restart()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f902lbceznE_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "classifier_br = MultiOutputLearner(\n",
        "    Perceptron()\n",
        ")\n",
        "evaluar(stream_original, classifier_br, 0.1)\n",
        "stream_original.restart()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eK_dz8cz6Ou_",
        "colab_type": "text"
      },
      "source": [
        "### Classifier Chains"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FX2inO297pyn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib notebook\n",
        "classifier_cc = ClassifierChain(\n",
        "    Perceptron()\n",
        ")\n",
        "evaluate_prequential(\n",
        "    stream_original, \n",
        "    classifier_cc,\n",
        "    0.1\n",
        ")\n",
        "stream_original.restart()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NflST1VkdeMl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "classifier_cc = ClassifierChain(\n",
        "    Perceptron()\n",
        ")\n",
        "evaluar(\n",
        "    stream_original, \n",
        "    classifier_cc,\n",
        "    0.1\n",
        ")\n",
        "stream_original.restart()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "S6pRGZ02-liT"
      },
      "source": [
        "### Label Combination Hoeffding Tree"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZeU2H2m6-lit",
        "colab": {}
      },
      "source": [
        "%matplotlib notebook\n",
        "classifier_lcht = LabelCombinationHoeffdingTreeClassifier(n_labels=stream_original.n_targets)\n",
        "evaluate_prequential(\n",
        "    stream_original, \n",
        "    classifier_lcht,\n",
        "    1000 / stream_original.n_remaining_samples()# READ(2012) recomienda entrenar con 1000 ejemplos (o cualquier valor del rango 500-1500)\n",
        ")\n",
        "stream_original.restart()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LqcOu3Ij-skC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "classifier_lcht = LabelCombinationHoeffdingTreeClassifier(n_labels=stream_original.n_targets)\n",
        "evaluar(\n",
        "    stream_original, \n",
        "    classifier_lcht,\n",
        "    1000 / stream_original.n_remaining_samples()# READ(2012) recomienda entrenar con 1000 ejemplos (o cualquier valor del rango 500-1500)\n",
        ")\n",
        "stream_original.restart()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LTtzoo72djM1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cat /proc/sys/vm/overcommit_memory\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHrv_KDwY3Mp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}